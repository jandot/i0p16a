{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jandot/i0p16a/blob/main/unsupervised_learning_clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unsupervised Learning: clustering"
      ],
      "metadata": {
        "id": "rZk75l5AhhF_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember to make your own copy of the file!\n",
        "\n",
        "\n",
        "`File` -> `Save a copy in drive`"
      ],
      "metadata": {
        "id": "hu8M26AnhiTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this section on clustering, please answer the questions that you'll encounter below in the following form: https://docs.google.com/forms/d/e/1FAIpQLSessAIZxYpx59AtmPssNFTn5J1ovWOkDyCq83-UP7QyzaWvxQ/viewform?usp=sf_link"
      ],
      "metadata": {
        "id": "RvPUb7YnoVQQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "In this part of unsupervised learning, we will focus on Clustering and apply some of methods on the Fisher iris dataset, and the dry bean dataset by Koklu & Ozkan (2020). We will use:\n",
        "\n",
        "* K-means\n",
        "* Hierarchical Clustering\n",
        "* HDBSCAN (It extends DBSCAN by converting it into a hierarchical clustering algorithm)\n",
        "\n",
        "\n",
        "## Datasets\n",
        "### Fisher iris dataset\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f6KbPXwksAliMIsibFyGJw.png\" width=500px >\n",
        "\n",
        "The iris dataset (https://www.kaggle.com/datasets/uciml/iris) used in a paper by Sir Ronald Fisher in 1936 describes 150 iris flowers, equally distributed across 3 species: Iris setosa, Iris versicolor and Iris virginica. For each flower, the following features are recorded:\n",
        "\n",
        "The data table looks like this:\n",
        "\n",
        "<div style=\"font-size: xx-small;\">\n",
        "\n",
        "| sepal_length | sepal_width | petal_length | petal_width | species |\n",
        "|--|--|--|--|--|\n",
        "| 5.1 | 3.5 | 1.4 | 0.2 | setosa |\n",
        "| 4.9 | 3.0 | 1.4 | 0.2 | setosa |\n",
        "| 4.7 | 3.2 | 1.3 | 0.2 | setosa |\n",
        "| 4.6 | 3.1 | 1.5 | 0.2 | setosa |\n",
        "| 5.0 | 3.6 | 1.4 | 0.2 | setosa |\n",
        "| 5.4 | 3.9 | 1.7 | 0.4 | setosa |\n",
        "| 4.6 | 3.4 | 1.4 | 0.3 | setosa |\n",
        "| 5.0 | 3.4 | 1.5 | 0.2 | setosa |\n",
        "| 4.4 | 2.9 | 1.4 | 0.2 | setosa |\n",
        "| ... | ... | ... | ... | ... |\n",
        "\n",
        "</div>\n",
        "\n",
        "### Beans dataset\n",
        "<img src=\"https://ars.els-cdn.com/content/image/1-s2.0-S0168169919311573-gr3.jpg\" width=500px />\n",
        "\n",
        "The beans dataset can be retrieved from https://www.kaggle.com/datasets/sansuthi/dry-bean-dataset and contains descriptions of 13,611 images of 7 different beans. These descriptions cover the following 16 features:\n",
        "\n",
        "1. Area (A): The area of a bean zone and the number of pixels within its boundaries.\n",
        "2. Perimeter (P): Bean circumference is defined as the length of its border.\n",
        "3. Major axis length (L): The distance between the ends of the longest line that can be drawn from a bean.\n",
        "4. Minor axis length (l): The longest line that can be drawn from the bean while standing perpendicular to the main axis.\n",
        "5. Aspect ratio (K): Defines the relationship between L and l.\n",
        "6. Eccentricity (Ec): Eccentricity of the ellipse having the same moments as the region.\n",
        "7. Convex area (C): Number of pixels in the smallest convex polygon that can contain the area of a bean seed.\n",
        "8. Equivalent diameter (Ed): The diameter of a circle having the same area as a bean seed area.\n",
        "9. Extent (Ex): The ratio of the pixels in the bounding box to the bean area.\n",
        "10. Solidity (S): Also known as convexity. The ratio of the pixels in the convex shell to those found in beans.\n",
        "11. Roundness (R): Calculated with the following formula: (4piA)/(P^2)\n",
        "12. Compactness (CO): Measures the roundness of an object: Ed/L\n",
        "13. ShapeFactor1 (SF1)\n",
        "14. ShapeFactor2 (SF2)\n",
        "15. ShapeFactor3 (SF3)\n",
        "16. ShapeFactor4 (SF4)\n",
        "17. Class (Seker, Barbunya, Bombay, Cali, Dermosan, Horoz and Sira)\n",
        "\n",
        "_Reference: Koklu M and Ozkan IA (2020) “Multiclass Classification of Dry Beans Using Computer Vision and Machine Learning Techniques.” Computers and Electronics in Agriculture 174:105507._\n",
        "\n",
        "The data table looks like this:\n",
        "\n",
        "<div style=\"font-size: xx-small;\">\n",
        "\n",
        "| Area | Perimeter | MajorAxisLength | MinorAxisLength | AspectRation | Eccentricity | ConvexArea | EquivDiameter | Extent | Solidity | roundness | Compactness | ShapeFactor1 | ShapeFactor2 | ShapeFactor3 | ShapeFactor4 | Class |\n",
        "|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|\n",
        "| 28395 | 610.291 | 208.1781167 | 173.888747 | 1.197191424 | 0.549812187 | 28715 | 190.1410973 | 0.763922518 | 0.988855999 | 0.958027126 | 0.913357755 | 0.007331506 | 0.003147289 | 0.834222388 | 0.998723889 | SEKER |\n",
        "| 28734 | 638.018 | 200.5247957 | 182.7344194 | 1.097356461 | 0.411785251 | 29172 | 191.2727505 | 0.783968133 | 0.984985603 | 0.887033637 | 0.953860842 | 0.006978659 | 0.003563624 | 0.909850506 | 0.998430331 | SEKER |\n",
        "| 29380 | 624.11 | 212.8261299 | 175.9311426 | 1.209712656 | 0.562727317 | 29690 | 193.4109041 | 0.778113248 | 0.989558774 | 0.947849473 | 0.908774239 | 0.007243912 | 0.003047733 | 0.825870617 | 0.999066137 | SEKER |\n",
        "| 30008 | 645.884 | 210.557999 | 182.5165157 | 1.153638059 | 0.498615976 | 30724 | 195.4670618 | 0.782681273 | 0.976695743 | 0.903936374 | 0.928328835 | 0.007016729 | 0.003214562 | 0.861794425 | 0.994198849 | SEKER |\n",
        "| 30140 | 620.134 | 201.8478822 | 190.2792788 | 1.06079802 | 0.333679658 | 30417 | 195.896503 | 0.773098035 | 0.99089325 | 0.984877069 | 0.970515523 | 0.00669701 | 0.003664972 | 0.941900381 | 0.999166059 | SEKER |\n",
        "| 30279 | 634.927 | 212.5605564 | 181.5101816 | 1.171066849 | 0.52040066 | 30600 | 196.3477022 | 0.775688485 | 0.989509804 | 0.943851783 | 0.923725952 | 0.007020065 | 0.003152779 | 0.853269634 | 0.999235781 | SEKER |\n",
        "| 30477 | 670.033 | 211.0501553 | 184.0390501 | 1.146768336 | 0.489477894 | 30970 | 196.9886332 | 0.762401501 | 0.984081369 | 0.853079869 | 0.933373552 | 0.006924899 | 0.003242016 | 0.871186188 | 0.999048736 | SEKER |\n",
        "| 30519 | 629.727 | 212.9967551 | 182.7372038 | 1.165590535 | 0.513759558 | 30847 | 197.1243203 | 0.770681818 | 0.989366875 | 0.967109244 | 0.925480392 | 0.006979152 | 0.003158285 | 0.856513956 | 0.99834456 | SEKER |\n",
        "| 30685 | 635.681 | 213.5341452 | 183.1571463 | 1.165852108 | 0.51408086 | 31044 | 197.659696 | 0.771561479 | 0.988435769 | 0.954239808 | 0.925658498 | 0.00695891 | 0.00315155 | 0.856843654 | 0.998952981 | SEKER |\n",
        "| ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... |\n",
        "\n",
        "</div>"
      ],
      "metadata": {
        "id": "PwcyUZ_1Fpak"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting things up\n",
        "See https://scikit-learn.org/stable/install.html for instructions on how to install the scikit-learn library. We will also need seaborn for plotting. If they are not installed, you can install them straight from within this notebook using the \"bang\" command `!pip install scikit-learn seaborn`."
      ],
      "metadata": {
        "id": "1dSKIJqXM2za"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn seaborn"
      ],
      "metadata": {
        "id": "WpGrxQHQcwBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set_theme(style=\"ticks\")"
      ],
      "metadata": {
        "id": "bX2Tj32zXQSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 Fisher's iris dataset\n",
        "\n",
        "\n",
        "The data is available from http://vda-lab.github.io/assets/iris.csv."
      ],
      "metadata": {
        "id": "imwC3ws_XbkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_iris = pd.read_csv('http://vda-lab.github.io/assets/iris.csv')\n",
        "df_iris"
      ],
      "metadata": {
        "id": "1khBFL0xH2Qx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a reminder, let's make a scatterplot matrix of the numerical features, coloured by species. This will give us a first idea of the data so that we know what we're working with."
      ],
      "metadata": {
        "id": "0ULjPdWnIsJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(df_iris, hue=\"species\")"
      ],
      "metadata": {
        "id": "TzhcbcjgItLL",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll apply different clustering methods, so make sure to _first_ **go through the documentation at https://scikit-learn.org/stable/modules/clustering.html**\n",
        "\n",
        "IMPORTANT: The API documentation for scikit-learn (https://scikit-learn.org/stable/modules/classes.html) includes code snippets at the end of each page, which we will copy-paste-modify in our own notebook below.\n",
        "\n",
        "Note: sklearn works with numpy datastructures instead of pandas structures, so we will have to make sure to have the correct type for the rest of the notebook."
      ],
      "metadata": {
        "id": "scWsEfuhKIuh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the first 10 iris flowers\n",
        "df_iris.values[:10]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1MV1Koc1Krky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For our k-means clustering, we want to only use the numerical values (i.e. not the species), so we'll create a new dataframe X that only holds that data."
      ],
      "metadata": {
        "id": "xQBEN3ins0pP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_iris[['sepal_length','sepal_width','petal_length','petal_width']].values\n",
        "print(type(df_iris))\n",
        "print(type(X))\n",
        "print(X[:10])"
      ],
      "metadata": {
        "id": "sZXq3ksoKt4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 K-Means\n",
        "Here we'll explore K Means Clustering, which is an unsupervised clustering technique, that is, finding clusters in data based on the data attributes alone (not the labels).\n",
        "\n",
        "K Means is a relatively easy-to-understand algorithm. It searches for cluster centers which are the mean of the points within them, such that every point is closest to the cluster center it is assigned to.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iHrGHF0qXOvY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, we use scikit-learn's implementation to calculate the clusters. Check the documentation on the scikit-learn website! (see link above).\n",
        "\n",
        "After we have calculated the clusters, we add a column to the `df_iris` dataset that contains the cluster number for that particular datapoints."
      ],
      "metadata": {
        "id": "r-snwgH3IHFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters=3, random_state=0, n_init=\"auto\").fit(X)\n",
        "df_iris['kmeans_cluster'] = list(map(str, kmeans.labels_))\n",
        "\n",
        "df_iris"
      ],
      "metadata": {
        "collapsed": true,
        "id": "WUOJCOTf9Xxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the table above: do the clusters agree with the species labels?\n",
        "\n",
        "Let's also plot 2 features (e.g. `sepal_length` and `sepal_width`) and colour them by species (first plot) or cluster (second plot)."
      ],
      "metadata": {
        "id": "G0CWQZODtgCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "# First plot with hue as \"kmeans_cluster\"\n",
        "sns.scatterplot(data=df_iris, x=\"sepal_length\", y=\"sepal_width\", hue=\"kmeans_cluster\", ax=axes[0])\n",
        "axes[0].set_title(\"Plot with Species Hue\")\n",
        "\n",
        "# Second plot with hue as \"species\"\n",
        "sns.scatterplot(data=df_iris, x=\"sepal_length\", y=\"sepal_width\", hue=\"species\", ax=axes[1])\n",
        "axes[1].set_title(\"Plot with KMeans Cluster Hue\")\n",
        "\n",
        "# Show the plots\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xoPbOpZyuQTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What do the above 2 plots tell you about the quality of the clustering?"
      ],
      "metadata": {
        "id": "pWxm9FXkqPJt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adjust the `n_cluster` parameter to change the number of clusters in the cluster, and choose an appropriate number of clusters according to the characteristics of the data set."
      ],
      "metadata": {
        "id": "EP3XpW3BYxp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##your code here"
      ],
      "metadata": {
        "id": "v8kKDUOPtN3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question:\n",
        "\n",
        "•\t**E1**: Which K would you choose for the iris dataset?"
      ],
      "metadata": {
        "id": "6ZhEOgGWhS00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Hierarchical clustering\n",
        "\n",
        "There are four kinds of hierarchical clustering strategies:\n",
        "\n",
        "* ward: The default strategy, that is, the least variance method. It tends to merge the two clusters that make the variance within the combined cluster minimal.\n",
        "* complete: When calculating the distance between two clusters, consider the distance between the two samples that are furthest apart in the two clusters.\n",
        "* average: When calculating the distance between two clusters, consider the average of the distance between all samples in the two clusters.\n",
        "* single: When calculating the distance between two clusters, consider the distance between the two samples closest to each other in the two clusters\n",
        "\n",
        "\n",
        "#### Create a dendrogram"
      ],
      "metadata": {
        "id": "lZpPiOQiNjxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.cluster.hierarchy as sch\n",
        "dendrogram = sch.dendrogram(sch.linkage(X, method = 'ward'))\n",
        "plt.title('Dendrogram')\n",
        "plt.xlabel('Flowers')\n",
        "plt.ylabel('Euclidean distances')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TQ3M-ljGQLIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the actual clusters. Just like with k-means clustering, we will add the results to the `df_iris` dataframe."
      ],
      "metadata": {
        "id": "RyAcpTvYQNW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.cluster.hierarchy import fcluster\n",
        "\n",
        "depth_cut = 10\n",
        "y_hc = fcluster(sch.linkage(X, method = 'ward'), t=depth_cut, criterion='distance')\n",
        "\n",
        "\n",
        "df_iris['y_hc'] = y_hc\n",
        "df_iris"
      ],
      "metadata": {
        "id": "TufrS5_0QPLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Is the clustering generated by k-means the same as by hierarchical clustering? Check the table.\n",
        "\n",
        "We will also plot this.\n",
        "\n",
        "Actually, let's do a PCA as well so that we can plot all points nicely in 2D."
      ],
      "metadata": {
        "id": "ML2tgtp6ylM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "df_iris[\"pca1\"] = X_pca[:, 0]\n",
        "df_iris[\"pca2\"] = X_pca[:, 1]"
      ],
      "metadata": {
        "id": "zQUFP13OzUpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "sns.scatterplot(data=df_iris, x=\"pca1\", y=\"pca2\", hue=\"species\", ax=axes[0])\n",
        "axes[0].set_title(\"Plot with Species Hue\")\n",
        "\n",
        "sns.scatterplot(data=df_iris, x=\"pca1\", y=\"pca2\", hue=\"kmeans_cluster\", ax=axes[1])\n",
        "axes[1].set_title(\"kmeans\")\n",
        "\n",
        "sns.scatterplot(data=df_iris, x=\"pca1\", y=\"pca2\", hue=\"y_hc\", ax=axes[2])\n",
        "axes[2].set_title(\"hierarchical\")"
      ],
      "metadata": {
        "id": "og2AkLihxEGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Question : strategy\n",
        "\n",
        "Retry the same hierarchical clustering, but using complete or single linkage. How does that change your results?"
      ],
      "metadata": {
        "id": "k6Qg3RZYf2cV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##your code here"
      ],
      "metadata": {
        "id": "HEaDOHFkiy30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### We can use a loop to show four mode's result as one plot"
      ],
      "metadata": {
        "id": "u4OXXIU0ivPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "\n",
        "linkage_methods = ['single', 'complete', 'average', 'ward']\n",
        "colors = ['viridis', 'plasma', 'cividis', 'magma']\n",
        "\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "for i, method in enumerate(linkage_methods):\n",
        "    Z = linkage(X, method=method)\n",
        "\n",
        "    y_hc = fcluster(Z, t=3, criterion='maxclust')\n",
        "\n",
        "\n",
        "    plt.subplot(2, 2, i + 1)\n",
        "    plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_hc, cmap=colors[i], edgecolor='k', s=50)\n",
        "    plt.title(f\"{method.capitalize()} Linkage\")\n",
        "    plt.xlabel(\"PCA first\")\n",
        "    plt.ylabel(\"PCA second\")\n",
        "\n",
        "plt.suptitle(\"Results of different clustering strategies\")\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "92zajehPw2ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 HDBSCAN\n",
        " How HDBSCAN works (https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html). **Check out this website first**\n",
        "\n",
        "\n",
        "The best way to explain HDBSCAN is actually just use it and then go through the steps that occurred along the way teasing out what is happening at each step. So let's load up the hdbscan library and get to work."
      ],
      "metadata": {
        "id": "Td6DnG4bfpkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hdbscan"
      ],
      "metadata": {
        "id": "uLM4dtgWpW-o",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import hdbscan"
      ],
      "metadata": {
        "id": "8vTJca52inQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clusterer = hdbscan.HDBSCAN(min_cluster_size=5, gen_min_span_tree=True)\n",
        "cluster_labels = clusterer.fit_predict(X)"
      ],
      "metadata": {
        "id": "Z-J-4KXGpGqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, HDBSCAN uses the Euclidean distance to see which datapoints are neighbours. But there are many. From https://hdbscan.readthedocs.io/en/latest/basic_hdbscan.html:\n",
        "\n",
        "```\n",
        "{'braycurtis': hdbscan.dist_metrics.BrayCurtisDistance,\n",
        " 'canberra': hdbscan.dist_metrics.CanberraDistance,\n",
        " 'chebyshev': hdbscan.dist_metrics.ChebyshevDistance,\n",
        " 'cityblock': hdbscan.dist_metrics.ManhattanDistance,\n",
        " 'dice': hdbscan.dist_metrics.DiceDistance,\n",
        " 'euclidean': hdbscan.dist_metrics.EuclideanDistance,\n",
        " 'hamming': hdbscan.dist_metrics.HammingDistance,\n",
        " 'haversine': hdbscan.dist_metrics.HaversineDistance,\n",
        " 'infinity': hdbscan.dist_metrics.ChebyshevDistance,\n",
        " 'jaccard': hdbscan.dist_metrics.JaccardDistance,\n",
        " 'kulsinski': hdbscan.dist_metrics.KulsinskiDistance,\n",
        " 'l1': hdbscan.dist_metrics.ManhattanDistance,\n",
        " 'l2': hdbscan.dist_metrics.EuclideanDistance,\n",
        " 'mahalanobis': hdbscan.dist_metrics.MahalanobisDistance,\n",
        " 'manhattan': hdbscan.dist_metrics.ManhattanDistance,\n",
        " 'matching': hdbscan.dist_metrics.MatchingDistance,\n",
        " 'minkowski': hdbscan.dist_metrics.MinkowskiDistance,\n",
        " 'p': hdbscan.dist_metrics.MinkowskiDistance,\n",
        " 'pyfunc': hdbscan.dist_metrics.PyFuncDistance,\n",
        " 'rogerstanimoto': hdbscan.dist_metrics.RogersTanimotoDistance,\n",
        " 'russellrao': hdbscan.dist_metrics.RussellRaoDistance,\n",
        " 'seuclidean': hdbscan.dist_metrics.SEuclideanDistance,\n",
        " 'sokalmichener': hdbscan.dist_metrics.SokalMichenerDistance,\n",
        " 'sokalsneath': hdbscan.dist_metrics.SokalSneathDistance,\n",
        " 'wminkowski': hdbscan.dist_metrics.WMinkowskiDistance}\n",
        "```"
      ],
      "metadata": {
        "id": "lifpKQkK1Vgg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This gives us the actual clustering in the variable `cluster_labels`. Why not add it to `df_iris` as well? (Exercise to do yourself)"
      ],
      "metadata": {
        "id": "BNZb-Ksj1zQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_labels"
      ],
      "metadata": {
        "id": "5R4sCIwb1CZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The underlying steps in HDBSCAN\n",
        "You normally wouldn't do this (because we already have the cluster assignments), but we can have a look at the different steps of HDBSCAN separately. What we want to show you here, is that in `scikit-learn` as well as `hdbscan`, there is much more information generated than just the output labels: we have access to the minimal spanning tree, the condensed tree, etc."
      ],
      "metadata": {
        "id": "-Hk-A0R71-j_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Build the minimum spanning tree\n",
        "\n",
        "We can build the minimum spanning tree very efficiently using Prim's algorithm -- we build the tree one edge at a time, always adding the lowest weight edge that connects the current tree to a vertex not yet in the tree. You can see the tree HDBSCAN constructed below; note that this is the **minimum spanning tree for mutual reachability distance** which is different from the pure distance in the graph. In the code block above, we use a k value of 5."
      ],
      "metadata": {
        "id": "iwjLeDEehdBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clusterer.minimum_spanning_tree_.plot(edge_cmap='viridis',\n",
        "                   edge_alpha=0.6,\n",
        "                   node_size=80,\n",
        "                   edge_linewidth=2)"
      ],
      "metadata": {
        "id": "adnhTztchcF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Build the cluster hierarchy\n",
        "\n",
        "Given the minimal spanning tree, the next step is to convert that into the hierarchy of connected components. This is most easily done in the reverse order: sort the edges of the tree by distance (in increasing order) and then iterate through, creating a new merged cluster for each edge. The only difficult part here is to identify the two clusters each edge will join together, but this is easy enough via a union-find data structure. We can view the result as a dendrogram as we see below:\n",
        "\n"
      ],
      "metadata": {
        "id": "sK8D_bx0aY3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clusterer.single_linkage_tree_.plot(cmap='viridis', colorbar=True)"
      ],
      "metadata": {
        "id": "GH5DcLpbjtXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Condense the cluster tree\n",
        "\n",
        "The first step in cluster extraction is condensing down the large and complicated cluster hierarchy into a smaller tree with a little more data attached to each node. As you can see in the hierarchy above it is often the case that a cluster split is one or two points splitting off from a cluster; and that is the key point -- rather than seeing it as a cluster splitting into two new clusters we want to view it as a single persistent cluster that is 'losing points'. To make this concrete we need a notion of minimum cluster size which we take as a parameter to HDBSCAN. Once we have a value for minimum cluster size we can now walk through the hierarchy and at each split ask if one of the new clusters created by the split has fewer points than the minimum cluster size. If it is the case that we have fewer points than the minimum cluster size we declare it to be 'points falling out of a cluster' and have the larger cluster retain the cluster identity of the parent, marking down which points 'fell out of the cluster' and at what distance value that happened. If on the other hand the split is into two clusters each at least as large as the minimum cluster size then we consider that a true cluster split and let that split persist in the tree. After walking through the whole hierarchy and doing this we end up with a much smaller tree with a small number of nodes, each of which has data about how the size of the cluster at that node descreases over varying distance. We can visualize this as a dendrogram similar to the one above -- again we can have the width of the line represent the number of points in the cluster. This time, however, that width varies over the length of the line as points fall our of the cluster. For our data using a minimum cluster size of 5 the result looks like this:"
      ],
      "metadata": {
        "id": "TzyWwy1mkHaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clusterer.condensed_tree_.plot()"
      ],
      "metadata": {
        "id": "Zo9JgN-FkPFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Extract the clusters\n",
        "\n",
        "select the clusters in the plot with the largest total ink area"
      ],
      "metadata": {
        "id": "SFKCYpTBkO3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clusterer.condensed_tree_.plot(select_clusters=True, selection_palette=sns.color_palette())"
      ],
      "metadata": {
        "id": "I0w5Elq8kkaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_iris[\"hdbscan\"] = clusterer.labels_\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "sns.scatterplot(data=df_iris, x=\"pca1\", y=\"pca2\", hue=\"species\", ax=axes[0])\n",
        "axes[0].set_title(\"Plot with Species Hue\")\n",
        "\n",
        "sns.scatterplot(data=df_iris, x=\"pca1\", y=\"pca2\", hue=\"hdbscan\", ax=axes[1])\n",
        "axes[1].set_title(\"hdbscan\")"
      ],
      "metadata": {
        "id": "bI7g02ggkw_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Is this what you expect?"
      ],
      "metadata": {
        "id": "bVFKnGnO3FCw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Question : Parameter Selection\n",
        "\n",
        "#####Selecting `min_cluster_size`\n",
        "The primary parameter to effect the resulting clustering is `min_cluster_size`. Ideally this is a relatively intuitive parameter to select – set it to the smallest size grouping that you wish to consider a cluster. It can have  effects however.\n",
        "\n",
        "##### Selecting `min_samples`\n",
        "\n",
        "The simplest intuition for what `min_samples` does is provide a measure of how conservative you want your clustering to be. The larger the value of `min_samples` you provide, the more conservative the clustering – more points will be declared as noise, and clusters will be restricted to progressively more dense areas.\n",
        "\n",
        "#####Selecting `cluster_selection_epsilon`\n",
        "The choice of `cluster_selection_epsilon` depends on the given distances between your data points. For example, set the value to 0.5 if you don’t want to separate clusters that are less than 0.5 units apart. This will basically extract DBSCAN* clusters for epsilon = 0.5 from the condensed cluster tree, but leave HDBSCAN* clusters that emerged at distances greater than 0.5 untouched.\n",
        "\n",
        "We can see those changes in practice."
      ],
      "metadata": {
        "id": "7qcZxN1YrMlW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##your code here"
      ],
      "metadata": {
        "id": "VHpWN1vRtj3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "•\t**E4**: why does the Accuracy of HDBSCAN bad? Can you fix it?"
      ],
      "metadata": {
        "id": "rjLZZM1Fu5Tu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "•\t**E5**: what clustering algorithms should you be using?"
      ],
      "metadata": {
        "id": "AWincZ1avnzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_beans_full = pd.read_csv('http://vda-lab.github.io/assets/Dry_Bean_Dataset.csv')\n",
        "\n",
        "# Remove the features that are combinations of other features\n",
        "df_beans = df_beans_full[['Area','Perimeter','MajorAxisLength','MinorAxisLength','Eccentricity','ConvexArea','EquivDiameter','Extent','Solidity','Class']]\n",
        "\n",
        "# Take a sample of 500\n",
        "# df_beans = df_beans.sample(n=500)\n",
        "df_beans"
      ],
      "metadata": {
        "id": "OeV3iXHCiZje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(df_beans[:100], hue=\"Class\")"
      ],
      "metadata": {
        "id": "faGTTLJJojtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All points have the same colour. Is that what you'd expect?"
      ],
      "metadata": {
        "id": "AiVOlzwi3o8I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the data clustering, we can only have numerical values, so we will remove the \"Class\" variable."
      ],
      "metadata": {
        "id": "v_Zbte8Bo5k2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_beans[['Area','Perimeter','MajorAxisLength','MinorAxisLength','Eccentricity','ConvexArea','EquivDiameter','Extent','Solidity']].values\n",
        "print(type(df_beans))\n",
        "print(type(X))"
      ],
      "metadata": {
        "id": "Y8q6a8GhpDVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That is your beans dataset pre-processed. Go ahead and perform k-means, hierarchical and HDBSCAN clustering yourself. Do not forget to adjust parameters as necessary and to see what their impact is."
      ],
      "metadata": {
        "id": "SYIrTN_w2hE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE"
      ],
      "metadata": {
        "id": "h93KEL5YhN99"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}